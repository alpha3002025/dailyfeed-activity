package click.dailyfeed.activity.domain.member.activity.consumer;

import click.dailyfeed.activity.domain.member.activity.document.MemberActivityDocument;
import click.dailyfeed.activity.domain.member.activity.mapper.MemberActivityMapper;
import click.dailyfeed.activity.domain.member.activity.repository.mongo.MemberActivityMongoRepository;
import click.dailyfeed.activity.domain.member.activity.repository.mongo.MemberActivityMongoTemplate;
import click.dailyfeed.code.domain.activity.factory.MemberActivityTransferDtoFactory;
import click.dailyfeed.code.domain.activity.transport.MemberActivityTransportDto;
import click.dailyfeed.code.domain.activity.type.MemberActivityType;
import click.dailyfeed.code.global.kafka.exception.KafkaNetworkErrorException;
import click.dailyfeed.code.global.kafka.type.DateBasedTopicType;
import click.dailyfeed.code.global.redis.RedisKeyExistPredicate;
import click.dailyfeed.deadletter.domain.deadletter.document.KafkaListenerDeadLetterDocument;
import click.dailyfeed.deadletter.domain.deadletter.repository.mongo.KafkaListenerDeadLetterMongoTemplate;
import click.dailyfeed.deadletter.domain.deadletter.repository.mongo.KafkaListenerDeadLetterRepository;
import click.dailyfeed.kafka.domain.activity.redis.KafkaMessageKeyMemberActivityRedisService;
import click.dailyfeed.redis.global.deadletter.kafka.MemberActivityEventRedisService;
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.support.Acknowledgment;
import org.springframework.kafka.support.KafkaHeaders;
import org.springframework.messaging.handler.annotation.Header;
import org.springframework.messaging.handler.annotation.Payload;
import org.springframework.scheduling.annotation.Scheduled;
import org.springframework.stereotype.Component;
import org.springframework.transaction.annotation.Propagation;
import org.springframework.transaction.annotation.Transactional;

import java.time.Duration;
import java.time.LocalDate;
import java.time.format.DateTimeFormatter;
import java.util.List;

@Slf4j
@RequiredArgsConstructor
@Component
@Transactional
public class MemberActivityEventConsumer {
    private final MemberActivityEventRedisService memberActivityEventRedisService;
    private final KafkaMessageKeyMemberActivityRedisService kafkaMessageKeyMemberActivityRedisService;

    private final MemberActivityMongoRepository memberActivityMongoRepository;
    private final KafkaListenerDeadLetterRepository kafkaListenerDeadLetterRepository;
    private final MemberActivityMongoTemplate memberActivityMongoTemplate;
    private final KafkaListenerDeadLetterMongoTemplate kafkaListenerDeadLetterMongoTemplate;

    private final MemberActivityMapper memberActivityMapper;
    private final ObjectMapper objectMapper;

    private final Duration KAFKA_LISTENER_TTL = Duration.ofSeconds(30);

    @KafkaListener(
            topicPattern = DateBasedTopicType.MEMBER_ACTIVITY_PATTERN,
            groupId = "member-activity-consumer-group-activity-svc",
            containerFactory = "memberActivityKafkaListenerContainerFactory"
    )
    public void consumeAllMemberActivityEvents(
            @Payload MemberActivityTransportDto.MemberActivityEvent event,
            @Header(KafkaHeaders.RECEIVED_TOPIC) String topic,
            @Header(KafkaHeaders.RECEIVED_PARTITION) int partition,
            @Header(KafkaHeaders.OFFSET) long offset,
            @Header(value = KafkaHeaders.RECEIVED_KEY, required = false) String messageKey,
            Acknowledgment acknowledgment) {

        // ì˜¤í”„ì…‹ ë° ì´ë²¤íŠ¸ ì •ë³´ ë¡œê¹…
        log.debug("ğŸ“¨ Consuming message - Topic: {}, Partition: {}, Offset: {}, MessageKey: {}, PostId: {}, EventType: {}",
                  topic, partition, offset, messageKey, event.getPostId(), event.getMemberActivityType());

        // Exactly Once ë¥¼ Off í•´ë‘ì—ˆê¸°ì— ì¤‘ë³µë©”ì‹œì§€ ìˆ˜ì‹  ê°€ëŠ¥, ì¤‘ë³µë©”ì‹œì§€ ì—¬ë¶€ ì²´í¬
        if (RedisKeyExistPredicate.EXIST.equals(kafkaMessageKeyMemberActivityRedisService.checkExist(messageKey))) {
            return;
        }

        try {
            // í† í”½ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
            String dateStr = DateBasedTopicType.MEMBER_ACTIVITY.extractDateFromTopicName(topic);

            // ì´ë²¤íŠ¸ ì²˜ë¦¬ (ëŒ€ê¸°ì—´ì— ì €ì¥)
            if (dateStr != null && dateStr.matches("\\d{8}")) { // ë‚ ì§œ íƒ€ì… ì²˜ë¦¬
                MemberActivityTransportDto.MemberActivityMessage message = MemberActivityTransportDto.MemberActivityMessage.builder()
                        .key(messageKey)
                        .event(event)
                        .build();

                processEventByDate(message, dateStr);
                // ë©”ì‹œì§€ ì²˜ë¦¬ ì„±ê³µ í›„ ì˜¤í”„ì…‹ ì»¤ë°‹
                acknowledgment.acknowledge();

                kafkaMessageKeyMemberActivityRedisService.addAndExpireIn(message.getKey(), KAFKA_LISTENER_TTL);
                log.debug("âœ… Offset committed - Topic: {}, Partition: {}, Offset: {}", topic, partition, offset);
            }


        } catch (Exception e) {
            log.error("âŒ Failed to process message - Topic: {}, Partition: {}, Offset: {}, Error: {}",
                     topic, partition, offset, e.getMessage(), e);
            handleListenException(MemberActivityTransportDto.MemberActivityMessage.builder().key(messageKey).event(event).build());
        }
    }

    /**
     * ë‚ ì§œë³„ ì´ë²¤íŠ¸ ì²˜ë¦¬
     */
    private void processEventByDate(MemberActivityTransportDto.MemberActivityMessage message, String dateStr) {
        LocalDate eventDate = LocalDate.parse(dateStr, DateTimeFormatter.ofPattern("yyyyMMdd"));
        LocalDate today = LocalDate.now();

        if (eventDate.equals(today)) {
            insertEvent(message);
        } else if (eventDate.isBefore(today)) { // ì‹œì°¨ë¡œ ì¸í•´ ì˜¤ëŠ˜ì—ì„œ ë‚´ì¼ë¡œ ë„˜ì–´ê°€ëŠ” ì¼€ì´ìŠ¤
            if(eventDate.isAfter(today.minusDays(2))) {
                insertEvent(message);
            }
            else{
                // ì ‘ë¯¸ì‚¬ê°€ yyyyMMdd í˜•ì‹ì´ ì•„ë‹Œ ë‹¤ë¥¸ í˜•ì‹ì˜ í† í”½ì¼ ê²½ìš° ì´ê³³ì—ì„œ ì²˜ë¦¬ (ìš´ì˜ì„ ìœ„í•œ íŠ¹ì • ìš©ë„)
            }
        } else {
            log.info("ë¯¸ë˜ì—ì„œ ì˜¤ì…¨êµ°ìš”, 10ë…„ ë’¤ì— ì‚¼ì„±ì „ì ì–¼ë§ˆì—ìš”?");
        }
    }

    @Transactional(propagation = Propagation.REQUIRED, rollbackFor = Exception.class)
    public void insertEvent(MemberActivityTransportDto.MemberActivityMessage message){
        final String messageKey = message.getKey();
        final MemberActivityTransportDto.MemberActivityEvent event = message.getEvent();
        if(!kafkaListenerDeadLetterRepository.findByMessageKey(messageKey).isEmpty()) return; // ì´ë¯¸ ë°ë“œë ˆí„°ì— ë‹´ì€ ë©”ì‹œì§€
        try{
            MemberActivityDocument document = memberActivityMapper.fromMessage(message);
            memberActivityMongoTemplate.upsertMemberActivity(document);
        } catch (Exception e) {
            DateTimeFormatter dateTimeFormatter = MemberActivityTransferDtoFactory.DATE_TIME_FORMATTER;
            String toRestore = String.format("messageKey=%s$$$memberId=%s$$$postId=%s$$$commentId=%s$$$activityType=%s$$$createdAt=%s$$$updatedAt%s$$$",
                    String.valueOf(messageKey), String.valueOf(event.getMemberId()), String.valueOf(event.getPostId()), String.valueOf(event.getCommentId()), event.getMemberActivityType().name(),
                    event.getCreatedAt().format(dateTimeFormatter), event.getUpdatedAt().format(dateTimeFormatter));
            try{ // deadletter ì €ì¥ì†Œì— ì €ì¥
                // object mapper ì§ë ¬í™”
                String payload = objectMapper.writeValueAsString(message);

                // deadletter ì €ì¥ì†Œì— ì €ì¥ ì‹œë„
                try {
                    MemberActivityType.Category category = MemberActivityType.resolveCategory(event.getMemberActivityType());
                    KafkaListenerDeadLetterDocument document = KafkaListenerDeadLetterDocument.newDeadLetter(messageKey, payload, category);
                    kafkaListenerDeadLetterMongoTemplate.upsertKafkaListenerDeadLetter(document);
                } catch (Exception e1){ // deadletter ì €ì¥ì†Œì— ì €ì¥ ì‹¤íŒ¨í•  ê²½ìš° PVC ë¡œê¹… ê¸°ëŠ¥ì„ í™œìš©
                    log.error(toRestore); // TODO :: íŠ¹ì • PVC ì— ë¡œê¹…í•˜ëŠ” ê¸°ëŠ¥ìœ¼ë¡œ ëŒ€ì²´ í•„ìš”
                }
            } catch (Exception e2){ // object mapper ë¥¼ ì´ìš©í•´ ê°ì²´ ì§ë ¬í™”ì— ì‹¤íŒ¨í•  ê²½ìš° (ì¹´í”„ì¹´ ë©”ì‹œì§€ ë²„ì „ì´ ë‹¬ë¼ì§€ëŠ” ì¼€ì´ìŠ¤)
                log.error(toRestore); // TODO :: íŠ¹ì • PVC ì— ë¡œê¹…í•˜ëŠ” ê¸°ëŠ¥ìœ¼ë¡œ ëŒ€ì²´ í•„ìš”
            }
        }
    }

    /// scheduled ê¸°ë°˜
    private void processLazy(MemberActivityTransportDto.MemberActivityMessage message) {
        // 1) Message read
        if (message == null) {
            return;
        }
        // 2) cache put
        memberActivityEventRedisService.rPushEvent(message);
        // 3) caching ì™„ë£Œëœ ë©”ì‹œì§€ í‚¤ ì €ì¥
        kafkaMessageKeyMemberActivityRedisService.addAndExpireIn(message.getKey(), KAFKA_LISTENER_TTL);
    }

    @Transactional(propagation = Propagation.REQUIRED, rollbackFor = Exception.class)
    @Scheduled(fixedRate = 1000, initialDelay = 5000)
    public void insertMany(){
        // 1ì´ˆì— í•œë²ˆì”© ë™ì‘ (ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ 5ì´ˆ í›„ë¶€í„°)
        int processedBatches = 0;

        while(true){
            List<MemberActivityTransportDto.MemberActivityMessage> eventList = memberActivityEventRedisService.lPopList();
            if(eventList == null || eventList.isEmpty()){
                break;
            }

            try{
                log.info("ğŸ“¦ Processing batch #{} - size: {}", ++processedBatches, eventList.size());
                List<MemberActivityDocument> list = eventList
                        .stream()
                        .map(memberActivityMapper::fromMessage)
                        .toList();

                // MongoDB ì €ì¥
                memberActivityMongoRepository.saveAll(list);
                log.debug("âœ… Successfully saved {} events to MongoDB", eventList.size());
            } catch (Exception e){
                log.error("âŒ Failed to save batch to MongoDB", e);

                // kafka DLQ publish
                eventList.stream().forEach(message -> {
                    handleListenException(message);
                });

                log.info("ğŸ“® Moved {} events to dead letter queue", eventList.size());
            }
        }
    }

    public void handleListenException(MemberActivityTransportDto.MemberActivityMessage message){
        final String messageKey = message.getKey();
        if(!kafkaListenerDeadLetterRepository.findByMessageKey(messageKey).isEmpty()) return; // ì´ë¯¸ ë°ë“œë ˆí„°ì— ë‹´ì€ ë©”ì‹œì§€

        MemberActivityTransportDto.MemberActivityEvent event = message.getEvent();
        try{
            String payload = objectMapper.writeValueAsString(message);
            try {
                MemberActivityType.Category category = MemberActivityType.resolveCategory(event.getMemberActivityType());
                KafkaListenerDeadLetterDocument document = KafkaListenerDeadLetterDocument.newDeadLetter(messageKey, payload, category);
                kafkaListenerDeadLetterMongoTemplate.upsertKafkaListenerDeadLetter(document);
            } catch (Exception e){
                log.error("");
            }
        }
        catch (JsonProcessingException e){
            // í†µì‹  ë©”ì‹œì§€ í˜•ì‹(v1, v2, ...) ì´ ë‹¤ë¥¸ ê²ƒìœ¼ë¡œ ì¸í•œ ì¥ì• ëŠ” í›„ë³´ì • ì„œë¹„ìŠ¤ì—ì„œ ì‹¤í–‰ë˜ë„ë¡í•˜ê³  ë„˜ì–´ê°„ë‹¤. ì• í”Œë¦¬ì¼€ì´ì…˜ ë ˆë²¨ì—ì„œ í•´ê²° ë¶ˆê°€ëŠ¥, ìš´ì˜ë ˆë²¨ì—ì„œ ê°œë°œìê°€ ëŒ€ì‘
            // ì—¬ê¸°ê¹Œì§€ ì‹¤íŒ¨í•  ê²½ìš° 'member-activity-yyyyMMdd í† í”½ì„ ì „ì¼ì í† í”½ìœ¼ë¡œ ëŒë©´ì„œ í›„ë³´ì • ì„œë¹„ìŠ¤ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ ìµì…‰ì…˜ì„ ë‚´ê³  ê·¸ëŒ€ë¡œ ë‘”ë‹¤.
            throw new KafkaNetworkErrorException();
        }
    }
}
